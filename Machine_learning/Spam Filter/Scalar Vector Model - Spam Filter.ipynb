{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VbRoFCGACDZd"
      },
      "outputs": [],
      "source": [
        "# Author: Kevin Richard \n",
        "# tensorflow for Spam Filter\n",
        "# Scalar Vector Model\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cEi8fCCUCanh",
        "outputId": "b84961b6-d285-40b8-a955-cf1c2ab50569"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2023-02-09 01:47:02--  http://=/\n",
            "Resolving = (=)... failed: Name or service not known.\n",
            "wget: unable to resolve host address ‘=’\n",
            "--2023-02-09 01:47:02--  https://archive.ics.uci.edu/ml/machine-learning-databases/spambase/spambase.data\n",
            "Resolving archive.ics.uci.edu (archive.ics.uci.edu)... 128.195.10.252\n",
            "Connecting to archive.ics.uci.edu (archive.ics.uci.edu)|128.195.10.252|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 702942 (686K) [application/x-httpd-php]\n",
            "Saving to: ‘spambase.data’\n",
            "\n",
            "spambase.data       100%[===================>] 686.47K   850KB/s    in 0.8s    \n",
            "\n",
            "2023-02-09 01:47:03 (850 KB/s) - ‘spambase.data’ saved [702942/702942]\n",
            "\n",
            "FINISHED --2023-02-09 01:47:03--\n",
            "Total wall clock time: 1.4s\n",
            "Downloaded: 1 files, 686K in 0.8s (850 KB/s)\n"
          ]
        }
      ],
      "source": [
        "#Download and import the spambase.data from the url.\n",
        "!wget = https://archive.ics.uci.edu/ml/machine-learning-databases/spambase/spambase.data\n",
        "filename = 'spambase.data'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GNVJbfjRKwLG"
      },
      "outputs": [],
      "source": [
        "# from google.colab import files\n",
        "# files.upload()  # Choose File \"spambase.data\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kCo3b8oCCicU"
      },
      "outputs": [],
      "source": [
        "# Tarining and Testing data allocation.\n",
        "testRatio = 0.5\n",
        "dataset = np.loadtxt(filename, dtype='float32', delimiter=',')\n",
        "target= dataset[:, -1]\n",
        "labels = LabelEncoder().fit_transform(target)\n",
        "features = dataset[:, 0:-1]\n",
        "feature_std = StandardScaler().fit_transform(features)\n",
        "x_train, x_test, y_train, y_test = train_test_split(feature_std, labels, test_size= testRatio, random_state = 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lVJlpF9DCz5P",
        "outputId": "c266d1ef-b05c-4c42-ffe3-06aead2ec4c8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train data size : 2300\n",
            "Number of Features : 57\n",
            "Test data size : 2301\n",
            "Number of Features : 57\n"
          ]
        }
      ],
      "source": [
        "#Train & Test data set check size and features.\n",
        "train_size, num_features = x_train.shape\n",
        "test_size, test_features = x_test.shape\n",
        "\n",
        "print(f\"Train data size : {train_size}\")\n",
        "print(f\"Number of Features : {num_features}\")\n",
        "print(f\"Test data size : {test_size}\")\n",
        "print(f\"Number of Features : {test_features}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QGY-YqKnDeW_"
      },
      "outputs": [],
      "source": [
        "#Support Vector Model\n",
        "class SupportVectorModel(object):\n",
        "\n",
        "    def __init__(self, num_features):\n",
        "        self.W = tf.Variable(tf.random.normal(shape=[num_features, 1]))\n",
        "        self.b = tf.Variable(tf.random.normal(shape=[1, 1]))\n",
        "        self.C = 0.001\n",
        "\n",
        "    def __call__(self, X):\n",
        "        return self.likelihood(X)\n",
        "\n",
        "    def likelihood(self, X):\n",
        "      raw_Y = tf.matmul(X, self.W) + self.b\n",
        "      return raw_Y\n",
        "\n",
        "\n",
        "    def predict(self, X):\n",
        "      p = self.likelihood(X)\n",
        "      return p\n",
        "\n",
        "    # the loss function\n",
        "    def compute_loss(self,y_pred, y_true):\n",
        "      c_loss = 0.5 * tf.matrix_square_root(tf.matmul(tf.transpose(self.W),self.W))\n",
        "      hinge_loss = tf.maximum(0, 1 - y_true * y_pred)\n",
        "      loss = self.C*c_loss + hinge_loss\n",
        "\n",
        "      return tf.reduce_mean(loss)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4k-TOaxzGHDO"
      },
      "outputs": [],
      "source": [
        "# accuracy function\n",
        "def accuracy(Output_values,y_data):\n",
        "  accuracy1 = 0\n",
        "  # number of correct predictions\n",
        "  for i in range(len(y_data)):\n",
        "    if Output_values[i] >= 0:\n",
        "      predicted_y = 1\n",
        "    else:\n",
        "      predicted_y = 0\n",
        "    eq = tf.cast(tf.equal(predicted_y, y_test[i]), tf.float32)  # eq is 0 if model does not predict correctly, or 1 if correctly\n",
        "    accuracy1 = accuracy1 + eq.numpy()\n",
        "\n",
        "  return accuracy1/len(y_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aL7D6E4BDsYr"
      },
      "outputs": [],
      "source": [
        "# Define a training loop:\n",
        "def training_loop(epochs,BATCH_SIZE,learning_rate):\n",
        "  epoch_plot = []\n",
        "  loss_plot = []\n",
        "  model = SupportVectorModel(num_features)\n",
        "  for i in epochs:\n",
        "    rand_index = np.random.choice(train_size, size = BATCH_SIZE) # generate 100 random indices\n",
        "    X = x_train[rand_index]  # given 100 random indices, choose 100 data points from train_data\n",
        "    Y = np.transpose([y_train[rand_index]])  # get their true y values for those 100 data points\n",
        "    Y = tf.reshape(Y, (-1, 1))  #new\n",
        "\n",
        "    Y_label = np.where(Y <= 0, -1, 1)\n",
        "\n",
        "    with tf.GradientTape() as tape:\n",
        "      loss = model.compute_loss(model(X), Y_label) # compute the loss based on the model output and the true Y\n",
        "\n",
        "    dW, db = tape.gradient(loss, [model.W, model.b])\n",
        "    model.W.assign_sub(learning_rate * dW)\n",
        "    model.b.assign_sub(learning_rate * db)\n",
        "\n",
        "    # print epoch and loss value every 100th iteration.\n",
        "    if i % 10 == 0:\n",
        "      print(\"=> epoch %2d: loss= %.2f\" %(i, loss.numpy()))\n",
        "      epoch_plot.append(i)\n",
        "      loss_plot.append(loss.numpy())\n",
        "\n",
        "\n",
        "##############  training is finished !!  ################3\n",
        "\n",
        "  # predict values for x_test data set\n",
        "  predicted_values = model.predict(x_test) # predicted labels are probabilities\n",
        "\n",
        "  # Accuracy of test Data:\n",
        "  print(\"accuracy:\",accuracy(predicted_values,y_test))\n",
        "  return accuracy(predicted_values,y_test),epoch_plot,loss_plot\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gPUzjoC14Xl0"
      },
      "outputs": [],
      "source": [
        "# epoch vs loss plot\n",
        "def epoch_loss_plot(epoch_plot1, loss_plot1, lr_1, epoch_plot2, loss_plot2, lr_2, epoch_plot3, loss_plot3, lr_3):\n",
        "  plt.plot(epoch_plot1, loss_plot1, label = f'{lr_1}', linewidth=2, color='blue')\n",
        "  plt.plot(epoch_plot2, loss_plot2, label = f'{lr_2}', linewidth=2, color='orange')\n",
        "  plt.plot(epoch_plot3, loss_plot3, label = f'{lr_3}', linewidth=2, color='green')\n",
        "  plt.xlabel('epoch')\n",
        "  plt.ylabel(\"loss\")\n",
        "  plt.title(f\"epoch vs loss\")\n",
        "  plt.locator_params(axis='x', nbins=12)\n",
        "  plt.legend(title = 'Learning Rate')\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mx3lgq3wDwAo"
      },
      "outputs": [],
      "source": [
        "# for epoch vs loss plots for varying Learning Rates\n",
        "\n",
        "ep_1 = 500\n",
        "bs_1 = 100\n",
        "lr_1 = 1000\n",
        "accuracy1, epoch_plot1, loss_plot1 = training_loop(range(ep_1),bs_1,lr_1)\n",
        "ep_2 = 500\n",
        "bs_2 = 100\n",
        "lr_2 = 100\n",
        "accuracy2, epoch_plot2, loss_plot2 = training_loop(range(ep_2),bs_2,lr_2)\n",
        "ep_3 = 500\n",
        "bs_3 = 100\n",
        "lr_3 = 1\n",
        "accuracy3, epoch_plot3, loss_plot3 = training_loop(range(ep_3),bs_3,lr_3)\n",
        "\n",
        "epoch_loss_plot(epoch_plot1, loss_plot1,lr_1, epoch_plot2, loss_plot2,lr_2, epoch_plot3, loss_plot3,lr_3)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-mT8cwXMQhkG"
      },
      "outputs": [],
      "source": [
        "#Learning rate vs accuracy plot\n",
        "\n",
        "def lr_accuracy_plot(accuracies, epos):\n",
        "  plt.plot(epos, accuracies, linewidth=2, color='blue')\n",
        "  plt.xlabel('lr')\n",
        "  plt.ylabel(\"accuracy\")\n",
        "  plt.title(f\"lr vs accuracy\")\n",
        "  plt.locator_params(axis='x', nbins=12)\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "CSSNW8JVPxAs"
      },
      "outputs": [],
      "source": [
        "# for epoch vs loss plots for varying Learning Rates\n",
        "\n",
        "accuracies=[]\n",
        "lrs = []\n",
        "epo = 1000\n",
        "lr = 1\n",
        "for i in range(10):\n",
        "  lr = lr + i*10\n",
        "  acc, _ , _ = training_loop(range(epo),100,lr)\n",
        "  accuracies.append(acc)\n",
        "  lrs.append(lr)\n",
        "\n",
        "print(\"Logistic Regression\")\n",
        "lr_accuracy_plot(accuracies, lrs )\n",
        "\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
